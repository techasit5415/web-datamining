import time
import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
import lightgbm as lgb
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, f1_score,r2_score
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.feature_selection import SelectKBest, chi2, mutual_info_regression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.linear_model import LinearRegression, Ridge, Lasso

st.title("Sleep Health and Lifestyle")

with st.spinner("Training models and generating plot...", show_time=True):
    start_time = time.time()
    file_path = "C:\\Users\\techa\\Downloads\\sleep_health_lifestyle_dataset.csv"

    # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å Drive
    df = pd.read_csv(file_path, encoding='latin1', low_memory=False) #
    st.write(df)
    st.write(df.isnull().sum(),df.info())

    #------------------------------------------------------------------------------------------------------

    #data cleaning
    df['Sleep Disorder'].unique()

    #------------------------------------------------------------------------------------------------------

    df['Sleep Disorder'].replace(np.nan, 'None', inplace=True)

    #------------------------------------------------------------------------------------------------------

    df['Sleep Disorder'].unique()

    #------------------------------------------------------------------------------------------------------

    df.isnull().sum()

    #------------------------------------------------------------------------------------------------------

    # ‡∏•‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏≤‡∏¢‡∏∏ < 0 ‡∏´‡∏£‡∏∑‡∏≠ > 120
    df = df[(df['Age'] >= 0) & (df['Age'] <= 120)]

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•‡∏Ç‡∏≠‡∏á Daily Steps ‡πÅ‡∏•‡∏∞ Physical Activity Level
    df = df[(df['Physical Activity Level (minutes/day)'] > 0) | (df['Daily Steps'] == 0)]

    #------------------------------------------------------------------------------------------------------

    df = df.drop(columns=['Person ID'])

    # One-Hot Encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó (Gender, Occupation, BMI Category, Sleep Disorder)
    df_encoded = pd.get_dummies(df, columns=['Gender', 'Occupation', 'BMI Category', 'Sleep Disorder'], drop_first=True)

    # ‡πÅ‡∏¢‡∏Å‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏ô‡πÇ‡∏•‡∏´‡∏¥‡∏ï (Blood Pressure) ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô Systolic BP ‡πÅ‡∏•‡∏∞ Diastolic BP
    df_encoded[['Systolic BP', 'Diastolic BP']] = df_encoded['Blood Pressure (systolic/diastolic)'].str.split('/', expand=True).astype(float)

    # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    df_encoded = df_encoded.drop(columns=['Blood Pressure (systolic/diastolic)'])

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    st.write((df_encoded.info()),(df_encoded.head()))
    print(df_encoded.info())
    print(df_encoded.head())

    #Data reduction (Feature Selection)


    #------------------------------------------------------------------------------------------------------
    # ‡πÅ‡∏¢‡∏Å X ‡πÅ‡∏•‡∏∞ y
    X = df_encoded.drop(columns=['Quality of Sleep (scale: 1-10)'])
    y = df_encoded['Quality of Sleep (scale: 1-10)']

    # ‡πÉ‡∏ä‡πâ RandomForest ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X, y)

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 8 ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    feature_importance = pd.Series(rf.feature_importances_, index=X.columns)
    important_features_rf = feature_importance.nlargest(10).index

    print(f"Selected Features (RandomForest): {list(important_features_rf)}")

    # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    X_selected_rf = X[important_features_rf]

    #------------------------------------------------------------------------------------------------------

    # ‡πÉ‡∏ä‡πâ XGBoost ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
    xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)
    xgb_model.fit(X, y)

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 10 ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    feature_importance = pd.Series(xgb_model.feature_importances_, index=X.columns)
    important_features_xgb = feature_importance.nlargest(10).index

    print(f"Selected Features (XGBoost): {list(important_features_xgb)}")

    # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    X_selected_xgb = X[important_features_xgb]

    #------------------------------------------------------------------------------------------------------

    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 80% ‡πÄ‡∏ó‡∏£‡∏ô, 20% ‡πÄ‡∏ó‡∏™
    X_train, X_test, y_train, y_test = train_test_split(X_selected_xgb, y, test_size=0.2, random_state=42)

    # Standardization (‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    #------------------------------------------------------------------------------------------------------

    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á
    rf_params = {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    # ‡πÉ‡∏ä‡πâ GridSearch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ params ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  
    rf_grid = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
    rf_grid.fit(X_train_scaled, y_train)

    # Train Model ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    rf_best = rf_grid.best_estimator_
    y_pred_rf = rf_best.predict(X_test_scaled)

    # Evaluate
    rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
    r2_rf = r2_score(y_test, y_pred_rf)

    print(f"RandomForest RMSE: {rmse_rf:.4f}, R¬≤: {r2_rf:.4f}")

    #------------------------------------------------------------------------------------------------------

    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö XGBoost
    xgb_params = {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 6, 10],
        'learning_rate': [0.01, 0.1, 0.3],
        'subsample': [0.6, 0.8, 1.0]
    }

    # ‡πÉ‡∏ä‡πâ GridSearch
    xgb_grid = GridSearchCV(xgb.XGBRegressor(random_state=42), xgb_params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
    xgb_grid.fit(X_train_scaled, y_train)

    # Train Model ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    xgb_best = xgb_grid.best_estimator_
    y_pred_xgb = xgb_best.predict(X_test_scaled)

    # Evaluate
    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
    r2_xgb = r2_score(y_test, y_pred_xgb)

    print(f"XGBoost RMSE: {rmse_xgb:.4f}, R¬≤: {r2_xgb:.4f}")

    #------------------------------------------------------------------------------------------------------

    #Data transformation
    #‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏™‡∏Å‡∏•
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_selected_xgb)

    #------------------------------------------------------------------------------------------------------

    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train/test
    X_train, X_test, y_train, y_test = train_test_split(X_train_scaled, y, test_size=0.2, random_state=42)

    # 1. XGBoost
    xgb_model = xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)
    xgb_model.fit(X_train, y_train)
    y_pred_xgb = xgb_model.predict(X_test)
    rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
    r2_xgb = r2_score(y_test, y_pred_xgb)

    # 2. LightGBM
    lgb_model = lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, random_state=42)
    lgb_model.fit(X_train, y_train)
    y_pred_lgb = lgb_model.predict(X_test)
    rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))
    r2_lgb = r2_score(y_test, y_pred_lgb)


    # 3. RandomForest
    rf_model = RandomForestRegressor(n_estimators=200, random_state=42)
    rf_model.fit(X_train, y_train)
    y_pred_rf = rf_model.predict(X_test)
    rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
    r2_rf = r2_score(y_test, y_pred_rf)



    # 5. Linear Regression
    lr_model = LinearRegression()
    lr_model.fit(X_train, y_train)
    y_pred_lr = lr_model.predict(X_test)
    rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
    r2_lr = r2_score(y_test, y_pred_lr)

    # 6. Ridge Regression
    ridge_model = Ridge(alpha=1.0)
    ridge_model.fit(X_train, y_train)
    y_pred_ridge = ridge_model.predict(X_test)
    rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))
    r2_ridge = r2_score(y_test, y_pred_ridge)


    # 7. Lasso Regression
    from sklearn.linear_model import LassoCV
    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ alpha ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Cross-Validation
    lasso_cv = LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, random_state=42)
    lasso_cv.fit(X_train, y_train)

    # ‡πÉ‡∏ä‡πâ alpha ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    best_alpha = lasso_cv.alpha_

    lasso_model = Lasso(alpha=best_alpha)
    lasso_model.fit(X_train, y_train)
    y_pred_lasso = lasso_model.predict(X_test)
    print(f"Best alpha: {best_alpha}")
    rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))
    r2_lasso = r2_score(y_test, y_pred_lasso)




    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• RMSE ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á DataFrame
    results_df = pd.DataFrame({
        "Model": ["XGBoost", "LightGBM", "Random Forest", "Linear Regression", "Ridge Regression", "Lasso Regression"],
        "RMSE": [rmse_xgb, rmse_lgb, rmse_rf, rmse_lr, rmse_ridge, rmse_lasso],
        "R¬≤ Score": [r2_xgb, r2_lgb, r2_rf, r2_lr, r2_ridge, r2_lasso]
    })

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    print(results_df)
    st.write(f"Best alpha for Lasso: {best_alpha}")

    #------------------------------------------------------------------------------------------------------


    reg_models = {
        "XGBoost": xgb_model,
        "LightGBM": lgb_model,
        "Random Forest": rf_model,
        "Linear Regression": lr_model,
        "Ridge Regression": ridge_model,
        "Lasso Regression": lasso_model
    }
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤ Actual vs Predicted ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Regression

    fig, ax = plt.subplots(figsize=(10, 7))
    for model_name, model in reg_models.items():
        model.fit(X_train, y_train)  # ‡πÉ‡∏ä‡πâ X_train ‡πÅ‡∏•‡∏∞ y_train ‡πÅ‡∏ó‡∏ô
        y_pred = model.predict(X_test)  # ‡πÉ‡∏ä‡πâ X_test ‡πÅ‡∏ó‡∏ô

        # Plot ‡∏à‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á
        plt.scatter(y_test, y_pred, alpha=0.6, label=f"{model_name} Predictions")
        ax.scatter(y_test, y_pred, alpha=0.6, label=f"{model_name} Predictions")

    # Plot ‡πÄ‡∏™‡πâ‡∏ô‡∏≠‡∏∏‡∏î‡∏°‡∏Ñ‡∏ï‡∏¥ (Ideal Line)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label="Ideal")
    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label="Ideal")

    # plt.xlabel("Actual Values")
    # plt.ylabel("Predicted Values")
    # plt.title("Regression: Actual vs Predicted (All Models)")
    # plt.legend()
    # plt.grid()
    ax.set_xlabel("Actual Values")
    ax.set_ylabel("Predicted Values")
    ax.set_title("Regression: Actual vs Predicted (All Models)")
    ax.legend()
    ax.grid(True)
    st.pyplot(fig)

    #------------------------------------------------------------------------------------------------------

    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏≤‡∏ü
    fig, ax = plt.subplots(1, 2, figsize=(12, 5))

    # üîπ Bar Plot ‡∏Ç‡∏≠‡∏á RMSE
    sns.barplot(data=results_df, x="Model", y="RMSE", ax=ax[0], palette="Blues_r")
    ax[0].set_title("Comparison of RMSE for Regression Models")
    ax[0].set_ylabel("RMSE")
    ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=30, ha="right")

    # üîπ Bar Plot ‡∏Ç‡∏≠‡∏á R¬≤ Score
    sns.barplot(data=results_df, x="Model", y="R¬≤ Score", ax=ax[1], palette="Greens_r")
    ax[1].set_title("Comparison of R¬≤ Score for Regression Models")
    ax[1].set_ylabel("R¬≤ Score")
    ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=30, ha="right")

    plt.tight_layout()
    
    st.pyplot(fig)

    #------------------------------------------------------------------------------------------------------
    

